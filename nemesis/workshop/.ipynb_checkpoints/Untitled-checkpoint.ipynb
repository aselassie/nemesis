{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def read_xml(link):\n",
    "    r = requests.get(f\"{link}\")\n",
    "    doc = r.text\n",
    "    print(link)\n",
    "    try:\n",
    "        root = etree.fromstring(doc)\n",
    "    except ValueError:\n",
    "        xml = bytes(bytearray(doc, encoding='utf-8'))\n",
    "        root = etree.XML(xml)\n",
    "    key = list(root.nsmap.keys())[0]\n",
    "    ns = f\"{{{root.nsmap[key]}}}\"\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for child in root:\n",
    "        name_of_issuer = child.find(f\"{ns}nameOfIssuer\").text\n",
    "        title_of_class = child.find(f\"{ns}titleOfClass\").text\n",
    "        cusip = child.find(f\"{ns}cusip\").text\n",
    "        value = child.find(f\"{ns}value\").text\n",
    "        number_of_shares = child.find(f\"{ns}shrsOrPrnAmt\").find(f\"{ns}sshPrnamt\").text\n",
    "        sh = child.find(f\"{ns}shrsOrPrnAmt\").find(f\"{ns}sshPrnamtType\").text\n",
    "        investment_discretion = child.find(f\"{ns}investmentDiscretion\").text\n",
    "        voting_authority_sole = child.find(f\"{ns}votingAuthority\").find(f\"{ns}Sole\").text\n",
    "        voting_authority_shared = child.find(f\"{ns}votingAuthority\").find(f\"{ns}Shared\").text\n",
    "        voting_authority_none = child.find(f\"{ns}votingAuthority\").find(f\"{ns}None\").text\n",
    "\n",
    "        single_row = dict(\n",
    "            name_of_issuer=name_of_issuer,\n",
    "            title_of_class=title_of_class,\n",
    "            cusip=cusip,\n",
    "            value=value,\n",
    "            number_of_shares=number_of_shares,\n",
    "            sh=sh,\n",
    "            investment_discretion=investment_discretion,\n",
    "            voting_authority_sole=voting_authority_sole,\n",
    "            voting_authority_shared=voting_authority_shared,\n",
    "            voting_authority_none=voting_authority_none\n",
    "        )\n",
    "        all_rows.append(single_row)\n",
    "    output_df = pd.DataFrame(all_rows)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def add_ticker_to_filing(raw_13f, cusip_map):\n",
    "    mapped_13_f = raw_13f.set_index(\"cusip\").join(cusip_map)\n",
    "    return mapped_13_f.reset_index()\n",
    "\n",
    "\n",
    "def get_links_from_cik(cik):\n",
    "    hostname = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "    link = f\"{hostname}?action=getcompany&CIK={cik}&type=13F-HR%25&dateb=&owner=include&start=0&count=40&output=atom\"\n",
    "    r = requests.get(f\"{link}\")\n",
    "    doc = r.text\n",
    "    root = etree.fromstring(bytes(doc, encoding='utf-8'))\n",
    "    ns = f\"{{{root.nsmap[None]}}}\"\n",
    "    entries = root.findall(f\"{ns}entry\")\n",
    "    all_links = []\n",
    "    for entry in entries:\n",
    "        single_entry_link = entry.find(f\"{ns}content\").find(f\"{ns}filing-href\").text\n",
    "        all_links.append(single_entry_link)\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def get_xml_links_from_table_link(single_link):\n",
    "    soup = BeautifulSoup(requests.get(single_link).text, 'html.parser')\n",
    "    table = soup.find(\"table\")\n",
    "    col_names = [col.text for col in table.findAll(\"th\")]\n",
    "    col_names\n",
    "    all_rows = []\n",
    "    for row in table.findAll(\"tr\"):\n",
    "        single_row = []\n",
    "        for i, cell in enumerate(row.findAll(\"td\")):\n",
    "            if i == 2:\n",
    "                single_row.append((cell.find(\"a\", href=True)['href']))\n",
    "                single_row.append(cell.find(\"a\", href=True).text)\n",
    "            else:\n",
    "                single_row.append(cell.text)\n",
    "        all_rows.append(single_row)\n",
    "    all_rows = pd.DataFrame(all_rows).dropna(how=\"all\")\n",
    "    all_rows.loc[:, 6] = [s[-1] for s in all_rows.iloc[:, 3].str.split(\".\")]\n",
    "    xml_rows = all_rows[all_rows.iloc[:, 6] == 'xml']\n",
    "    try:\n",
    "        infotable_link =  xml_rows[xml_rows.iloc[:, 4].str.upper() == 'INFORMATION TABLE'].iloc[0, 2]\n",
    "    except IndexError:\n",
    "        file_format = all_rows[all_rows.iloc[:, 4].str.upper().str.contains('13F')].iloc[0, 6]\n",
    "        if file_format == 'txt':\n",
    "            return {\"infotable_link\": None, \"primary_doc_link\": None}\n",
    "    primary_doc_link = xml_rows[xml_rows.iloc[:, 4].str.upper().str.contains('13F')].iloc[0, 2]\n",
    "    infotable_link = f\"https://www.sec.gov{infotable_link}\"\n",
    "    primary_doc_link = f\"https://www.sec.gov{primary_doc_link}\"\n",
    "    return {\"infotable_link\": infotable_link, \"primary_doc_link\": primary_doc_link}\n",
    "\n",
    "\n",
    "def get_date_from_xml(link):\n",
    "    r = requests.get(link)\n",
    "    doc = r.text\n",
    "    root = etree.fromstring(bytes(doc, encoding='utf-8'))\n",
    "    ns = f\"{{{root.nsmap[None]}}}\"\n",
    "    report_date = root.find(f'{ns}headerData').find(f\"{ns}filerInfo\").find(f\"{ns}periodOfReport\").text\n",
    "    return report_date\n",
    "\n",
    "\n",
    "def read_xml_dictionary(links_dictionary):\n",
    "    data_output = read_xml(links_dictionary['infotable_link']).set_index(\"cusip\")\n",
    "    report_date = get_date_from_xml(links_dictionary['primary_doc_link'])\n",
    "    data_output.loc[:, \"report_date\"] = report_date\n",
    "    return data_output\n",
    "\n",
    "\n",
    "def get_all_filings(cik, last_n=None):\n",
    "    single_fund_collection = []\n",
    "    all_links = get_links_from_cik(cik)[:last_n]\n",
    "    for link in all_links:\n",
    "        single_dictionary = get_xml_links_from_table_link(link)\n",
    "        single_fund_collection.append(single_dictionary)\n",
    "\n",
    "    all_filings = []\n",
    "    for single_dictionary in single_fund_collection:\n",
    "        single_df = read_xml_dictionary(single_dictionary)\n",
    "        all_filings.append(single_df)\n",
    "    all_filings = pd.concat(all_filings)\n",
    "    all_filings.loc[:, \"report_date\"] = pd.to_datetime(all_filings.report_date).dt.date\n",
    "    return all_filings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_links_from_cik(\"0001317583\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = get_links_from_cik(\"0001317583\")\n",
    "for link in all_links:\n",
    "    single_dictionary = get_xml_links_from_table_link(link)\n",
    "    single_fund_collection.append(single_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
